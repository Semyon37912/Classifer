## В этом задании нужно было предсказать кто из пасажиров переместится в измерении, а кто нет.

### Для начала провел EDA:
1.Разбил признак "Cabin" на 2 признака: "Deck" - номер пасубы и "Side" - расположение каюты слева или справа.
2.Преобразовал категориальные признаки в числовые.
3.Для заполнения пропусков я использовал метод "KNN".
4.Для удаления выбросов использовал цикл из методов "Local Outlier Factor".

### Подготовил данные для машинного обучения:
1.Передал переменной X нецелевые признаки, а переменной y целевой.
2.Разбил данные на тренировочные и тестовые.
3.Провел нормализацию данных.

### Обучил модели на данных:
1.Обучил модели на тренировочной выборке: 
"Decision Tree", "SVM", "Naive Bayes", "Logistic Regression"

2.Поверил качество классификации на тестовой с помощью:
?...

3.Качество предсказания было уже приемлемым, но для улучшения качества решил использовать ансамбли: 
"Staking", "Boosting", "Bagging", "XGBoost", "Random Forest"

4.Для нахождения наилучших гиперпараметров "Random Forest", использовал метод "Randomized Search".

### Вывод:
Наилучшим образом паказал себя ансамбль "Random Forest", именно на нем и было принято решение делать финальное предсказание.

## Алгоритмы работы методов использованных в этой работе:
### Заполнение пропусков с помощью KNN - 
  Когда KNN используется для заполнения пропущенных значений, сначала определяется расстояние между
наблюдениями с помощью какой-либо метрики расстояния, например, евклидова или Манхэттенская метрики. Затем находятся k ближайших соседей
с известными значениями для данной переменной. Значение пропущенной переменной затем вычисляется путем усреднения или медианы значений
переменной у найденных k ближайших соседей.

### Поиск выбросов с помощью Local Outlier Factor - 
Алгоритм LOF использует KNN для определения соседей каждой точки. Алгоритм начинает с
определения k ближайших соседей каждой точки и вычисления расстояний до этих соседей. Затем для каждой точки вычисляется локальный
коэффициент выброса (local outlier factor), который определяет степень отличия плотности точки от плотности ее соседей.
Локальный коэффициент выброса вычисляется путем сравнения плотности точки с плотностью ее соседей. Если плотность точки меньше
плотности ее соседей, то локальный коэффициент выброса больше единицы, что указывает на то, что точка скорее всего является выбросом.
Если плотность точки сравнима с плотностью ее соседей, то локальный коэффициент выброса близок к единице, что указывает на то, что точка
вероятно не является выбросом.

### Decision Tree -
Процесс построения дерева решений начинается с корневого узла, который представляет все доступные данные. Затем для каждого узла
производится разбиение данных на основе признаков, чтобы максимизировать различие между классами или минимизировать ошибку
прогнозирования. Это выполняется путем определения признака, который наиболее эффективно разделяет данные на классы. Эффективность
разбиения оценивается на основе различных метрик, таких как критерий Джини или энтропия Шеннона.
После разбиения данных на подмножества процесс повторяется для каждого подмножества, создавая новые внутренние узлы и ветви, пока не
будет достигнут критерий остановки. Критерии остановки могут включать максимальную глубину дерева, минимальное количество наблюдений в
каждом листе или минимальное значение ошибки.

### SVM -
В SVM каждый объект данных представлен точкой в многомерном пространстве, где каждая измеренная функция (признак) представляет отдельную
размерность. Затем SVM строит гиперплоскость, которая максимально разделяет точки, принадлежащие разным классам. Гиперплоскость является
линией (в случае двухмерного пространства) или гиперплоскостью (в случае пространств с большим количеством измерений), которая
максимизирует расстояние между точками данных разных классов, называемое зазором (margin).

Процесс построения гиперплоскости состоит из нахождения оптимального разделителя данных и определения векторов поддержки, которые
представляют точки данных, ближайшие к гиперплоскости. Эти векторы поддержки используются для определения зазора между классами и
настройки гиперплоскости для оптимальной классификации новых данных.

### Naive Bayes -
Теорема Байеса гласит, что вероятность того, что некоторое событие A произойдет, при условии, что произошло событие B, можно вычислить по
формуле:
P(A|B) = P(B|A) * P(A) / P(B),
где P(A) - априорная вероятность события A, P(B|A) - вероятность события B при условии, что произошло событие A, P(B) - полная
вероятность события B.

В Naive Bayes каждый объект описывается набором признаков, и каждый признак является независимым от других признаков. Это предположение называется "наивным" (naive), поскольку в реальности многие признаки могут быть зависимыми.

При классификации объекта на основе его признаков, Naive Bayes вычисляет вероятности каждого класса на основе априорной вероятности
класса и условной вероятности признаков для данного класса. Затем, используя формулу Байеса, определяется вероятность того, что объект
относится к каждому классу, и выбирается наиболее вероятный класс.

Для обучения Naive Bayes необходимо собрать набор данных с известными метками классов для каждого объекта. Используя этот набор данных,
Naive Bayes вычисляет априорную вероятность каждого класса и условную вероятность признаков для каждого класса. Затем, при классификации
нового объекта, Naive Bayes использует эти вероятности для определения класса объекта.

### Logistic Regression - 
Логистическая регрессия работает следующим образом:
1.На вход алгоритму подаются признаки объектов (features), которые описывают каждый объект. Например, это могут быть размер, цвет, вес и
т.д.
2.Алгоритм обучается на тренировочном наборе данных, где каждый объект имеет метку класса (class label), к которому он должен быть
отнесен. В процессе обучения, алгоритм находит оптимальные веса (weights) для каждого признака, которые позволяют правильно разделять
объекты на два класса.
3.После обучения, алгоритм использует найденные веса, чтобы прогнозировать класс новых объектов. Для этого, он вычисляет линейную
комбинацию входных признаков с найденными весами и применяет к результату сигмоидную функцию, которая преобразует полученное число в
вероятность принадлежности к классу 1.
4.Если вероятность превышает определенный порог, то объект относится к классу 1, иначе - к классу 0.

### Staking -
В стейкинге моделей используются несколько моделей, которые работают вместе над задачей, а не в отдельности. Каждая модель получает часть
входных данных и создает свой прогноз. Затем результаты объединяются в один общий прогноз, который считается более точным и устойчивым,
чем результаты, полученные от каждой модели по отдельности.

Стейкинг моделей может быть реализован с помощью различных алгоритмов. Например, можно использовать голосование с разными весами для
каждой модели, чтобы учитывать их индивидуальную точность. Также можно использовать алгоритмы, которые учитывают исторические результаты
каждой модели и динамически адаптируют вес каждой модели в зависимости от ее производительности.

### Bagging -
Основной шаг в методе Bagging - это создание нескольких независимых моделей, которые обучаются на разных случайных выборках данных,
созданных путем выбора примеров из исходного набора данных с возвращением (т.е. повторением). Затем каждая модель используется для
генерации предсказаний на новых данных, и результаты каждой модели комбинируются вместе, например, путем голосования или усреднения.

Преимущество метода Bagging заключается в том, что он может уменьшить влияние шума и выбросов в данных, повысить устойчивость модели к
изменениям в данных и снизить вероятность переобучения. Также Bagging позволяет использовать параллельную обработку для ускорения
обучения.

### Boosting - 
Основная идея состоит в том, чтобы последовательно обучать несколько слабых моделей на одних и тех же данных, используя веса для
предыдущих моделей, чтобы сконцентрироваться на тех примерах, которые были предсказаны неправильно ранее. Таким образом, каждая следующая
модель фокусируется на тех примерах, на которых предыдущие модели совершали ошибки, что позволяет улучшить качество общей модели.

В основе метода Boosting лежит алгоритм Gradient Boosting, который работает следующим образом:
Обучаем начальную модель на данных.
Оцениваем ошибки и веса каждого обучающего примера.
Обучаем следующую модель, фокусируясь на примерах с большими весами ошибок предыдущей модели.
Оцениваем ошибки и веса каждого обучающего примера с учетом предыдущих моделей.
Повторяем шаги 3 и 4 для каждой последующей модели.
Комбинируем результаты всех моделей для получения итоговой модели.

### XGBoost - 
Основная идея XGBoost - это построение ансамбля деревьев решений, которые обучаются последовательно, с использованием градиентного спуска
для минимизации функции потерь. При обучении XGBoost использует следующие ключевые элементы:

Деревья решений: каждая модель в ансамбле - это дерево решений, которое разбивает данные на несколько областей (листьев) на основе
значимых признаков и их значений.
Градиентный спуск: используется для минимизации функции потерь. Градиенты вычисляются на каждом шаге и используются для корректировки
параметров модели.
Регуляризация: используется для контроля переобучения модели путем добавления штрафов за сложность модели или за сильные веса признаков.
XGBoost также использует различные техники, такие как шумоподавление, сжатие данных, предварительное обучение и другие, чтобы улучшить
производительность модели.

### Random Forest - 
Основные шаги Random Forest:

Из исходного набора данных создается несколько случайных подвыборок, которые выбираются с возвращением (бутстрапирование).
На каждой подвыборке строится решающее дерево, при этом на каждом узле дерева случайным образом выбирается подмножество признаков,
которые будут использоваться для разделения узла. Это позволяет уменьшить корреляцию между деревьями и сделать их более разнообразными.
Для каждого нового примера данных каждое из деревьев в лесу дает предсказание. Для задач классификации предсказания каждого дерева
объединяются посредством голосования, а для задач регрессии - усредняются.
Преимущества метода Random Forest включают высокую точность и устойчивость к шуму и выбросам, а также возможность работать с большим
количеством признаков и классов. Это также дает возможность оценивать важность каждого признака для классификации или регрессии.

### Переобучение модели - 
Это явление, когда модель слишком сильно подстраивается под обучающие данные и теряет способность обобщать свои предсказания на новые
данные, которые не были использованы при обучении. Это может проявляться в том, что модель показывает очень высокую точность на обучающих
данных, но при этом плохо работает на новых данных или на тестовых данных.

Существует несколько методов борьбы с переобучением модели:
1.Регуляризация - добавление дополнительных ограничений на параметры модели, чтобы предотвратить их переобучение на обучающих данных.
Например, можно добавить к функции потерь модели штраф за большие значения параметров или ограничить максимальную длину весов.
2.Использование более простых моделей - если модель слишком сложная, то она может сильно подстраиваться под обучающие данные, даже если
это не необходимо для достижения хорошей производительности. Поэтому можно попробовать использовать более простые модели, например,
линейные модели или решающие деревья.
3.Увеличение размера обучающей выборки - если модель переобучается из-за того, что ей не хватает данных для обучения, то можно
попробовать увеличить размер обучающей выборки.
4.Использование методов сокращения размерности - если модель переобучается из-за того, что ей передаются слишком много признаков, можно
использовать методы сокращения размерности, такие как метод главных компонент (PCA) или методы отбора признаков, чтобы уменьшить
количество признаков, передаваемых модели.
5.Использование кросс-валидации - кросс-валидация может помочь оценить производительность модели на новых данных и проверить, насколько
она способна обобщать свои предсказания на новые данные. Она также может помочь определить оптимальные значения гиперпараметров модели,
которые могут помочь улучшить ее производительность.
6.Использование ранней остановки - ранняя остановка является методом регуляризации, который заключается в прекращении обучения модели, когда ошибка на тестовой выборке начинает увеличиваться. Это позволяет остановить обучение модели на ранней стадии, когда она начинает переобучаться.
